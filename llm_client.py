"""
LLMå®¢æˆ·ç«¯
æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€æ¥å£
"""
import asyncio
import json
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod
import openai
from config import settings

class LLMClient(ABC):
    """LLMå®¢æˆ·ç«¯æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆå“åº”"""
        pass
    
    @abstractmethod
    async def generate_structured_response(self, prompt: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»“æ„åŒ–å“åº”"""
        pass

class OpenAIClient(LLMClient):
    """OpenAIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: str, model: str = "gpt-3.5-turbo", base_url: Optional[str] = None):
        # å…è®¸è‡ªå®šä¹‰ base_urlï¼Œæœªæä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        self.client = openai.AsyncOpenAI(api_key=api_key, base_url=base_url or settings.openai_base_url)
        self.model = model
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆå“åº”"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 1000),
                top_p=kwargs.get("top_p", 1.0),
                frequency_penalty=kwargs.get("frequency_penalty", 0.0),
                presence_penalty=kwargs.get("presence_penalty", 0.0)
            )
            return response.choices[0].message.content
        except Exception as e:
            raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    async def generate_structured_response(self, prompt: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»“æ„åŒ–å“åº”"""
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›ç»“æœã€‚"},
                    {"role": "user", "content": f"{prompt}\n\nè¯·è¿”å›JSONæ ¼å¼çš„ç»“æœï¼Œç¬¦åˆä»¥ä¸‹schemaï¼š\n{json.dumps(schema, ensure_ascii=False, indent=2)}"}
                ],
                temperature=0.3,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©º
            if not content or not content.strip():
                raise Exception("LLMè¿”å›ç©ºå“åº”")
            
            # å°è¯•è§£æJSON
            try:
                return json.loads(content)
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSONè§£æå¤±è´¥: {str(e)}")
                print(f"ğŸ“ åŸå§‹å†…å®¹: {content[:200]}...")
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    try:
                        return json.loads(json_match.group())
                    except json.JSONDecodeError:
                        raise Exception("æ— æ³•è§£ææå–çš„JSONéƒ¨åˆ†")
                else:
                    raise Exception("æ— æ³•ä»å“åº”ä¸­æå–JSONå†…å®¹")
        except Exception as e:
            raise Exception(f"OpenAIç»“æ„åŒ–å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")

class MockLLMClient(LLMClient):
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    
    def __init__(self):
        self.responses = {
            "complexity_analysis": "complex",
            "task_planning": "éœ€è¦åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡",
            "execution": "æ‰§è¡Œç»“æœ",
            "writing": "ç”Ÿæˆçš„ç­”æ¡ˆå†…å®¹"
        }
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        # æ ¹æ®promptå†…å®¹è¿”å›ä¸åŒçš„æ¨¡æ‹Ÿå“åº”
        if "å¤æ‚åº¦" in prompt or "complexity" in prompt.lower():
            return "complex"
        elif "è§„åˆ’" in prompt or "planning" in prompt.lower():
            return "éœ€è¦åˆ†è§£ä¸ºå¤šä¸ªå­ä»»åŠ¡"
        elif "æ‰§è¡Œ" in prompt or "execution" in prompt.lower():
            return "æ‰§è¡Œç»“æœ"
        elif "ç”Ÿæˆ" in prompt or "writing" in prompt.lower():
            return "ç”Ÿæˆçš„ç­”æ¡ˆå†…å®¹"
        else:
            return "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„LLMå“åº”"
    
    async def generate_structured_response(self, prompt: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨¡æ‹Ÿç»“æ„åŒ–å“åº”"""
        await asyncio.sleep(0.5)
        
        # è¿”å›ç¬¦åˆschemaçš„æ¨¡æ‹Ÿæ•°æ®
        if "complexity" in schema:
            return {"complexity": "complex", "reason": "æŸ¥è¯¢åŒ…å«å¤šä¸ªå­é—®é¢˜"}
        elif "tasks" in schema:
            return {
                "tasks": [
                    {"id": "task1", "type": "search", "description": "æœç´¢ç›¸å…³ä¿¡æ¯"},
                    {"id": "task2", "type": "calculate", "description": "æ‰§è¡Œè®¡ç®—"}
                ],
                "dependencies": {"task2": ["task1"]}
            }
        else:
            return {"result": "æ¨¡æ‹Ÿç»“æ„åŒ–å“åº”"}

class AppIdLLMClient(LLMClient):
    """åŸºäºAppIdåˆ—è¡¨çš„LLMå®¢æˆ·ç«¯"""
    
    def __init__(self, appid_list: List[str], model: str = "gpt-3.5-turbo", base_url: Optional[str] = None):
        self.appid_list = appid_list
        self.model = model
        self.current_appid_index = 0
        self.failed_appids = set()  # è®°å½•å¤±è´¥çš„AppId
        self.base_url = base_url or settings.openai_base_url
    
    def _get_next_appid(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„AppIdï¼ˆè½®è¯¢æ–¹å¼ï¼‰"""
        if not self.appid_list:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„AppId")
        
        # è¿‡æ»¤æ‰å¤±è´¥çš„AppId
        available_appids = [appid for appid in self.appid_list if appid not in self.failed_appids]
        if not available_appids:
            # å¦‚æœæ‰€æœ‰AppIdéƒ½å¤±è´¥äº†ï¼Œé‡ç½®å¤±è´¥åˆ—è¡¨
            self.failed_appids.clear()
            available_appids = self.appid_list
        
        appid = available_appids[self.current_appid_index % len(available_appids)]
        self.current_appid_index = (self.current_appid_index + 1) % len(available_appids)
        return appid
    
    async def generate_response(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆå“åº”"""
        max_retries = settings.llm_max_retries
        base_wait = max(0.5, settings.llm_retry_wait_seconds)
        for attempt in range(max_retries):
            try:
                appid = self._get_next_appid()
                print(f"ğŸ”‘ ä½¿ç”¨AppId: {appid} (å°è¯• {attempt + 1}/{max_retries})")
                
                # å°è¯•è°ƒç”¨å®é™…çš„API
                response = await self._call_appid_api(prompt, appid, **kwargs)
                if response:
                    return response
                
                # å¦‚æœAPIè°ƒç”¨å¤±è´¥ï¼Œæ ‡è®°è¿™ä¸ªAppIdä¸ºå¤±è´¥
                self.failed_appids.add(appid)
                print(f"âš ï¸ AppId {appid} è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª")
                
            except Exception as e:
                print(f"âŒ AppId APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
                if attempt < max_retries - 1:
                    # æŒ‡æ•°é€€é¿ç­‰å¾…
                    wait_seconds = min(base_wait * (2 ** attempt), 60.0)
                    await asyncio.sleep(wait_seconds)
                else:
                    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œè¿”å›é™çº§å“åº”
                    return await self._fallback_response(prompt)
        
        return await self._fallback_response(prompt)
    
    async def generate_structured_response(self, prompt: str, schema: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»“æ„åŒ–å“åº”"""
        try:
            # å…ˆå°è¯•ç”Ÿæˆæ™®é€šå“åº”
            response = await self.generate_response(prompt)
            
            # å°è¯•è§£æä¸ºJSON
            import json
            try:
                if response and response.strip():
                    return json.loads(response)
            except json.JSONDecodeError:
                pass
            
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç¬¦åˆschemaçš„é™çº§æ•°æ®
            return self._generate_fallback_structured_response(schema)
                
        except Exception as e:
            print(f"âŒ AppIdç»“æ„åŒ–å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._generate_fallback_structured_response(schema)
    
    async def _call_appid_api(self, prompt: str, appid: str, **kwargs) -> Optional[str]:
        """è°ƒç”¨AppId API (é€šè¿‡ OpenAI å…¼å®¹æ¥å£ï¼Œä½¿ç”¨ appid ä½œä¸º api_key)"""
        try:
            # ä¸ºè¯¥ appid æ„å»ºä¸´æ—¶å®¢æˆ·ç«¯
            client = openai.AsyncOpenAI(api_key=appid, base_url=self.base_url)
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", 1000)
            top_p = kwargs.get("top_p", 1.0)
            frequency_penalty = kwargs.get("frequency_penalty", 0.0)
            presence_penalty = kwargs.get("presence_penalty", 0.0)

            try:
                response = await client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=top_p,
                    frequency_penalty=frequency_penalty,
                    presence_penalty=presence_penalty
                )
                return response.choices[0].message.content
            except Exception as e:
                # é™æµå¤„ç†ï¼šé‡åˆ°429æˆ–RateLimitç›¸å…³æŠ¥é”™æ—¶ï¼Œè¿”å›Noneè®©ä¸Šå±‚é‡è¯•
                err_text = str(e).lower()
                if "rate" in err_text or "429" in err_text or "limit" in err_text:
                    return None
                raise
        except Exception as e:
            print(f"âŒ AppId {appid} APIè°ƒç”¨å¼‚å¸¸: {str(e)}")
            return None
    
    async def _fallback_response(self, prompt: str) -> str:
        """é™çº§å“åº”"""
        print("ğŸ”„ ä½¿ç”¨é™çº§å“åº”ç­–ç•¥")
        return f"é™çº§å“åº”: æ— æ³•è·å–LLMæœåŠ¡ï¼Œä½¿ç”¨åŸºç¡€å¤„ç† - {prompt[:100]}..."
    
    def _generate_fallback_structured_response(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé™çº§çš„ç»“æ„åŒ–å“åº”"""
        if "complexity" in schema:
            return {"complexity": "unknown", "reason": "æ— æ³•è·å–LLMæœåŠ¡"}
        elif "tasks" in schema:
            return {
                "tasks": [
                    {"id": "fallback_task", "type": "search", "description": "é™çº§ä»»åŠ¡å¤„ç†"}
                ],
                "dependencies": {}
            }
        else:
            return {"result": "é™çº§å“åº”", "error": "LLMæœåŠ¡ä¸å¯ç”¨"}

class LLMClientFactory:
    """LLMå®¢æˆ·ç«¯å·¥å‚"""
    
    @staticmethod
    def create_client(provider: str = "openai", **kwargs) -> LLMClient:
        """åˆ›å»ºLLMå®¢æˆ·ç«¯"""
        if provider == "openai":
            api_key = kwargs.get("api_key") or settings.openai_api_key
            if not api_key:
                raise Exception("OpenAI APIå¯†é’¥æœªé…ç½®")
            return OpenAIClient(
                api_key=api_key,
                model=kwargs.get("model", "gpt-3.5-turbo"),
                base_url=kwargs.get("base_url") or settings.openai_base_url,
            )
        elif provider == "mock":
            return MockLLMClient()
        elif provider == "appid":
            # ä½¿ç”¨appid_listé…ç½®çš„å®¢æˆ·ç«¯
            appid_list = kwargs.get("appid_list") or settings.appid_list
            if not appid_list:
                raise Exception("appid_listæœªé…ç½®")
            return AppIdLLMClient(
                appid_list=appid_list,
                model=kwargs.get("model", "gpt-3.5-turbo"),
                base_url=kwargs.get("base_url") or settings.openai_base_url,
            )
        else:
            raise Exception(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")

# å…¨å±€LLMå®¢æˆ·ç«¯å®ä¾‹
llm_client: Optional[LLMClient] = None

def get_llm_client() -> LLMClient:
    """è·å–å…¨å±€LLMå®¢æˆ·ç«¯"""
    global llm_client
    if llm_client is None:
        # æ ¹æ®é…ç½®é€‰æ‹©å®¢æˆ·ç«¯ç±»å‹
        if settings.openai_api_key:
            provider = "openai"
        elif settings.appid_list:
            provider = "appid"
        else:
            provider = "mock"
        
        llm_client = LLMClientFactory.create_client(provider=provider)
    return llm_client

def set_llm_client(client: LLMClient):
    """è®¾ç½®å…¨å±€LLMå®¢æˆ·ç«¯"""
    global llm_client
    llm_client = client
